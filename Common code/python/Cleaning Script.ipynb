{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClientName FYxx Interim/Year End (Apr 2020 to Mar 2021) TestName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc \n",
    "conn = pyodbc.connect('Driver={ODBC Driver 17 for SQL Server};'\n",
    "                      'Server=;'\n",
    "                      'Database=;'\n",
    "                      'Trusted_Connection=yes;')\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "sql_query = pd.read_sql_query('SELECT top 100 * FROM dbo.',conn)\n",
    "#SELECT TOP 100 * FROM dbo.docFileApprovals \n",
    "#SELECT TOP 100 * FROM dbo.transactions_AU_p0_to_p5 \n",
    "#SELECT TOP 100 * FROM dbo.transactions_AU_p6_to_p10 \n",
    "#SELECT TOP 100 * FROM dbo.transactions_nonAU \n",
    "print(sql_query.shape,'\\n')\n",
    "sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xlrd==1.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import pyodbc\n",
    "from datetime import datetime as dt\n",
    "from typing import Union, List\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "#import PyPDF2\n",
    "#import xlrd\n",
    "#import camelot\n",
    "\n",
    "import dttlib\n",
    "from importlib import reload\n",
    "reload(dttlib)\n",
    "from dttlib.data.reading import read_data\n",
    "from dttlib.data.uploading import DataFrameUploader\n",
    "\n",
    "import swifter\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_path=Path(r'')\n",
    "input_path=master_path/'2.Precleaned'\n",
    "output_path=master_path/'3.Cleaned'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Load data from xlsx to dataframe directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lines=read_data(input_path/'IN',delimiter='\\t',header=None,encoding='windows-1252',error_bad_lines=False,engine='python')\n",
    "\n",
    "print(df_lines.shape,'\\n')\n",
    "\n",
    "df_lines.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=camelot.read_pdf(r''),\n",
    "#                    flavor='stream', pages='1',encoding = 'ISO-8859-1',table_areas=['0,800,800,00']\n",
    "#                   )\n",
    "df.parsing_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1].to_excel(input_path/'IN'/'DPR 953.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1].to_csv(input_path/'IN'/'DPR 953.csv', sep = '|', \n",
    "                   index=False#,quoting=csv.QUOTE_ALL\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_1=pd.concat(pd.read_excel(input_path/'IN'/'DPR 953.xlsx', sheet_name=None,header=None), ignore_index=True)\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_2=pd.concat(pd.read_excel(input_path/'IN'/'DPR 1005.xlsx', sheet_name=None,header=None), ignore_index=True)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.concat([df_1,df_2],ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_gl=pd.read_excel(input_path/'Errored rows.xlsx',dtype=str,index_col=None,header=0)\n",
    "\n",
    "print(type(df_gl),'\\n')\n",
    "\n",
    "print(df_gl.shape)\n",
    "\n",
    "df_gl.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gl['Short Item No'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=read_data(input_path/'OUT',file_extension='csv',delimiter='|',header=0)\n",
    "\n",
    "print(df_test.shape,'\\n') # 1,172,379\n",
    "\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(input_path/'Errored rows.csv', sep = '|',index=False)#,quoting=csv.QUOTE_ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gl[0]=df_gl[0].str.strip()\n",
    "df_gl['GL account']=df_gl[0].str[0:4]\n",
    "df_gl.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# importing required modules\n",
    "#import PyPDF2\n",
    "# \n",
    "## creating a pdf file object\n",
    "#pdfFileObj = open(input_path/'IN'/'DPR 953.pdf', 'rb')\n",
    "# \n",
    "## creating a pdf reader object\n",
    "#pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "# \n",
    "## printing number of pages in pdf file\n",
    "#print(pdfReader.numPages)\n",
    "# \n",
    "## creating a page object\n",
    "#pageObj = pdfReader.getPage(1)\n",
    "# \n",
    "## extracting text from page\n",
    "#print(pageObj.extractText())\n",
    "# \n",
    "## closing the pdf file object\n",
    "##pdfFileObj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gl.to_csv(input_path/'Payroll.csv', sep = '|', \n",
    "                   index=False#,quoting=csv.QUOTE_ALL\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "uploader=DataFrameUploader(server='', database='')\n",
    "uploader.upload_raw(df_lines,table_name='OPEX',initials='EL',pk=['DTT_ID'],overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_pay_desc=pd.read_excel(input_path/'20210617 (Non traded AP)'/'Payment Terms Description.xlsx',\n",
    "                      'Sheet2',dtype=str, index_col=None,header=0)\n",
    "\n",
    "print(type(df_pay_desc),'\\n')\n",
    "\n",
    "print(df_pay_desc.shape)\n",
    "\n",
    "df_pay_desc.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pay_desc.to_csv(output_path/'20210618_Load'/'Payment Terms Description.csv', sep = '|', \n",
    "                   index=False#,quoting=csv.QUOTE_ALL\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_ap=pd.read_excel(input_path/'20210617 (Non traded AP)'/'',\n",
    "                    '',dtype=str, index_col=None,header=0)\n",
    "\n",
    "print(type(df_ap),'\\n') # 640074\n",
    "\n",
    "print(df_ap.shape)\n",
    "\n",
    "df_ap.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ap.to_csv(output_path/'20210618_Load'/'', sep = '|', \n",
    "             index=False#,quoting=csv.QUOTE_ALL\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_open=pd.read_excel(input_path/'20210617 Open Invoices'/'',\n",
    "                    '',dtype=str, index_col=None,header=0)\n",
    "\n",
    "print(type(df_open),'\\n')\n",
    "\n",
    "print(df_open.shape)\n",
    "\n",
    "df_open.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_open.to_csv(output_path/'20210618_Load'/'Open Invoices not paid as at 17062021.csv', sep = '|', \n",
    "               index=False#,quoting=csv.QUOTE_ALL\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Union\n",
    "Union separate dataframe into 1 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union=pd.concat([df_xlsx,...]).reset_index(drop=True)\n",
    "\n",
    "print(df_union.shape,'\\n')\n",
    "df_union.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### export as .csv to precleaned folder if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union.to_csv(input_path/'Matching Names.csv',quoting=csv.QUOTE_ALL,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading with DTT_ID and DTT_FILENAME (For Projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lines=read_data(input_path/'test.csv',delimiter='|',header=0,encoding='utf-8'\n",
    "                   #error_bad_lines=False,engine='python'\n",
    "                  )\n",
    "\n",
    "print(df_lines.shape,'\\n')\n",
    "\n",
    "df_lines.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### checks where needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "uploader=DataFrameUploader(server='', database='')\n",
    "uploader.upload_raw(df_lines,table_name='ZZZ_TEST',initials='EL',pk=['DTT_ID'],overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Precheck\n",
    "Run quick checks on consistency, data range and completeness\n",
    "\n",
    "Aim for a very quick check before getting further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lines[''].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_lines.sort_values(by=[''],ascending = True)[''].unique() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check missing values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_lines.isna().sum()\n",
    "# crucial columns have no missing values, good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.to_datetime(df_lines['']).min(),pd.to_datetime(df_lines['']).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert amount to float\n",
    "df_lines['']=df_lines[''].astype('float') * df_lines['indicator'].apply(lambda x: -1 if x =='H' else 1) # if it's SAP\n",
    "round(df_lines[''].sum(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert journal line number to integer\n",
    "df_lines['']=df_lines[''].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sum by gl based on the raw data\n",
    "sum_by_gl_raw=df_lines.groupby('GL').agg({'Line number': 'count','Amount': 'sum'}\n",
    "                                         ).rename(columns = {'Line number':'Count','Amount':'Sum'})\n",
    "sum_by_gl_raw['Sum']=round(sum_by_gl_raw['Sum'],2)\n",
    "sum_by_gl_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export if needed\n",
    "sum_by_gl_raw.to_excel(output_path/'sum_by_gl_raw.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check data types for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df_lines.dtypes,'\\n')\n",
    "df_lines.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask_fix_1=df_lines['ACCOUNTED_CR'].str.contains('* Staff ', case=False, na=False, regex=False)\n",
    "#df_lines.loc[mask_fix_1,'ACCOUNTED_CR']=0.00\n",
    "#df_lines.loc[mask_fix_1,'DESCRIPTIO']=str(df_lines['ACCOUNTED_CR']) + ' ' + df_lines['DESCRIPTIO']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "Clean the data into proper format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### format date columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lines['']=pd.to_datetime(df_lines[''])\n",
    "df_lines['']=pd.to_datetime(df_lines[''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### format amount columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lines['']=df_lines[''].astype('float') * df_lines['D/C'].apply(lambda x: -1 if x =='H' else 1)\n",
    "df_lines['']=df_lines[''].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### form correct journal id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_lines['Journal ID']=df_lines[''] + '_' + df_lines[''] + '_' + ...\n",
    "df_lines['Journal ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### format column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lines.columns=df_lines.columns.str.title()\n",
    "df_lines.columns=df_lines.columns.str.upper()\n",
    "df_lines.columns=df_lines.columns.str.lower()\n",
    "df_lines.columns=df_lines.columns.str.replace('','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove leading and trailing space for all 'object' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lines[df_lines.select_dtypes(['object']).columns]=df_lines[df_lines.select_dtypes(['object']).columns].apply(lambda x: x.str.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove double quotes and replace NaN with blank for journal description column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lines['']=df_lines[''].str.replace('\"','')\n",
    "\n",
    "mask=df_lines[''].isnull()\n",
    "df_lines.loc[mask,'']=''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subset dataframe as per column mapping with required column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rename a few columns\n",
    "col_name_raw = ['','','']\n",
    "\n",
    "col_mapping = {'':'','':''}\n",
    "\n",
    "col_name_new = list(col_mapping.values())\n",
    "col_name_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_subset=df_lines[col_name_raw].rename(columns=col_mapping)[col_name_new]\n",
    "\n",
    "print(df_subset.shape,'\\n') # 9,056,899, rowcount maintained\n",
    "\n",
    "print(df_subset.dtypes) # correct data types for each column\n",
    "\n",
    "df_subset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validation\n",
    "Validate cleaned data from above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validate if journal netting to 0 and by each journal id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df_subset['Amount'].sum(),2)\n",
    "# netting to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net by [Journal ID]\n",
    "net_by_jID=round(df_subset.groupby('Journal ID').agg({'Journal line number':'count','Amount': 'sum'}),2)\n",
    "unbalanced=net_by_jID[net_by_jID['Amount'].abs()>0].rename(columns={'Journal line number':'Count','Amount':'Sum'})\n",
    "unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbalanced_lines=df_subset[df_subset['Journal ID'].isin(['','',''])]\n",
    "# export unbalanced lines\n",
    "unbalanced_lines.to_excel(output_path/'unbalanced_lines.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validate if each journal id has unique journal line number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_unique=df_subset.groupby(['Journal ID','Journal line number']).size()\n",
    "check_unique[check_unique>1].to_frame().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validate if each journal id has unique doc type, posting user, date effective and date posted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_jID_doc=df_subset.groupby('Journal ID').agg({'Document type': lambda w: w.nunique()})\n",
    "unique_jID_doc[unique_jID_doc['Document type']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unique_jID_user=df_subset.groupby('Journal ID').agg({'Posting user': lambda x: x.nunique()})\n",
    "unique_jID_user[unique_jID_user['Posting user']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_jID_e_date=df_subset.groupby('Journal ID').agg({'Date effective': lambda y: y.nunique()})\n",
    "unique_jID_e_date[unique_jID_e_date['Date effective']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_jID_p_date=df_subset.groupby('Journal ID').agg({'Date posted': lambda z: z.nunique()})\n",
    "unique_jID_p_date[unique_jID_p_date['Date posted']>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate if gl account in consistent format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_subset.sort_values(by=['GL account'], ascending = True)['GL account'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exporting\n",
    "Export Movement by GL.xlsx, Journal lines.csv, Document types.csv and Posting users.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate movement by gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum_by_gl=df_subset.groupby('GL account').agg({'Journal line number': 'count','Amount': 'sum'}\n",
    "                                             ).rename(columns = {'Journal line number':'Count','Amount':'Sum'})\n",
    "sum_by_gl['Sum']=round(sum_by_gl['Sum'],2)\n",
    "sum_by_gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "sum_by_gl.to_excel(output_path/'sum_by_gl.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop columns if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset=df_lines.drop(columns=['',''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate final journal line data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re order columns if needed\n",
    "df_final=df_subset#.reindex(columns=col_header)\n",
    "\n",
    "print(df_final.shape,'\\n') # , maintained, good\n",
    "\n",
    "df_final.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "df_final.to_csv(output_path/'.csv',quoting=csv.QUOTE_ALL,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate doc type data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_type=df_final.groupby('Document type').agg({'Journal ID': lambda x: x.nunique(),\n",
    "                                                'Journal line number': 'count',\n",
    "                                                'Amount': 'sum'}\n",
    "                                              ).rename(columns = {'Journal ID':'Number of entries',\n",
    "                                                                  'Journal line number':'Number of lines',\n",
    "                                                                  'Amount':'Total amount'}).reset_index()\n",
    "doc_type['Is standard document type'] = 0\n",
    "doc_type['Total amount']=round(doc_type['Total amount'],2)\n",
    "doc_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "doc_type.to_csv(output_path/'document_type.csv',quoting=csv.QUOTE_ALL,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate posting user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "posting_user=df_final.groupby('Posting user').agg({'Journal ID': lambda x: x.nunique(),\n",
    "                                                   'Journal line number': 'count',\n",
    "                                                   'Amount': 'sum'}\n",
    "                                                 ).rename(columns = {'Journal ID':'Number of entries',\n",
    "                                                                     'Journal line number':'Number of lines',\n",
    "                                                                     'Amount':'Total amount'}).reset_index()\n",
    "posting_user['Is system entry'] = 0\n",
    "posting_user['User of interest'] = 0\n",
    "posting_user['Total amount']=round(posting_user['Total amount'],2)\n",
    "posting_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "posting_user.to_csv(output_path/'posting_user.csv',quoting=csv.QUOTE_ALL,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revenue, OPEX, COS, Payroll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import xxx list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xxx_import=read_data(input_path/'revenue list',encoding='utf-8',header=0)\n",
    "\n",
    "print(df_xxx_import.shape,'\\n')\n",
    "\n",
    "df_xxx_import.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract xxx only journals from exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx_only=pd.merge(df_final,df_xxx_import,how='inner',# test out left join as well\n",
    "                  left_on=[''],right_on=[''],suffixes=('', '_y'))\n",
    "\n",
    "xxx_only=xxx_only[['','','','']] # or xxx_only=xxx_only.drop(columns=['',''])\n",
    "\n",
    "print(xxx_only.shape,'\\n')\n",
    "\n",
    "xxx_only.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx_only[(xxx_only['']>='') & (xxx_only['']<='')].to_csv(output_path/'.csv',quoting=csv.QUOTE_ALL,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx_only[(xxx_only['']>='') & (xxx_only['']<='')].to_excel(output_path/'.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract journal lines that touches xxx account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx_distinct_jIDs=pd.DataFrame(xxx_only['Journal ID'].unique())\n",
    "xxx_distinct_jIDs.shape # 733,998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx_all=pd.merge(xxx_distinct_jIDs,df_final, how='left',left_on=0,right_on='Journal ID')\n",
    "xxx_all.shape # 2,799,739"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx_all[(xxx_all['']>='') & (xxx_all['']<='')].to_csv(output_path/'.csv',quoting=csv.QUOTE_ALL,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx_all[(xxx_all['']>='') & (xxx_all['']<='')].to_excel(output_path/'.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
