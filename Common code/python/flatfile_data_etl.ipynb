{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:pink;\">üìä Flatfile Data ETL Script</h1>\n",
    "<h3 style=\"color:purple;\">This script involves: </h3>\n",
    "\n",
    "        üîç Detecting & Handling Excel File Formats\n",
    "        üîê Decrypting Encrypted Files\n",
    "        üìê Analyzing Workbook Structures\n",
    "        üîÑ Consolidating Data into DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Viewing Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import sas7bdat\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "#pd.set_option('display.width', None)\n",
    "#np.set_printoptions(threshold=np.inf)\n",
    "#pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    Read delimitered flatfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "def read_and_combine_csv(master_path):\n",
    "    all_csv_files = master_path.rglob('*.csv')  # Recursively find all CSV files #.parquet\n",
    "    combined_dfs = {}  # Dictionary to store combined DataFrames\n",
    "\n",
    "    for csv_file in all_csv_files:\n",
    "        df = pd.read_csv(csv_file,encoding='utf-8',index_col=False,header=0,dtype=str,delimiter=','\\\n",
    "                         ,quoting=csv.QUOTE_MINIMAL)\n",
    "#        df = pd.read_parquet(csv_file, engine='pyarrow')\n",
    "#        df = df.astype(str)\n",
    "        \n",
    "        # Add ID column at the beginning\n",
    "        df.insert(0, 'DTT_ID', range(1, len(df) + 1))\n",
    "\n",
    "        # Add Filename column at the end\n",
    "        df['DTT_FILENAME'] = csv_file.name\n",
    "\n",
    "        columns_tuple = tuple(df.columns)  # Tuple of column names for comparison\n",
    "\n",
    "        if columns_tuple in combined_dfs:\n",
    "            # Append to the existing DataFrame with the same columns\n",
    "            combined_dfs[columns_tuple] = pd.concat([combined_dfs[columns_tuple], df], ignore_index=True)\n",
    "        else:\n",
    "            # Create a new entry in the dictionary for these columns\n",
    "            combined_dfs[columns_tuple] = df\n",
    "\n",
    "    return combined_dfs\n",
    "\n",
    "# Usage\n",
    "master_folder_path = Path(r'')\n",
    "combined_dataframes = read_and_combine_csv(master_folder_path)\n",
    "\n",
    "# Displaying combined dataframes\n",
    "for i, df in enumerate(combined_dataframes.values()):\n",
    "    print(f\"DataFrame {i+1} Preview:\")\n",
    "    display(df.head(2))\n",
    "    print(f\"Total rows in DataFrame {i+1}: {len(df)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list = list(combined_dataframes.values())\n",
    "\n",
    "# Access the second DataFrame (remember, Python uses 0-based indexing)\n",
    "#second_dataframe = dataframes_list[1]\n",
    "\n",
    "# Now you can work with second_dataframe\n",
    "print(\"DataFrame Preview:\")\n",
    "display(dataframes_list[0].head(2))\n",
    "print(f\"Total rows in Second DataFrame: {len(dataframes_list[0])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Remove double quotes and trim trailing space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_quotes_and_trim_dataframes(all_dfs):\n",
    "    trimmed_dfs = []  # List to store cleaned DataFrames\n",
    "\n",
    "    for df in all_dfs:\n",
    "        # Create a copy to avoid modifying the original DataFrame\n",
    "        df_copy = df.copy()\n",
    "\n",
    "        # Select columns of object type\n",
    "        cols = df_copy.select_dtypes(['object']).columns\n",
    "\n",
    "        # Apply the string operations only to string values\n",
    "        for col in cols:\n",
    "            df_copy.loc[:, col] = df_copy[col].apply(lambda x: x.replace('\"', '').strip() if isinstance(x, str) else x)\n",
    "        \n",
    "        trimmed_dfs.append(df_copy)\n",
    "\n",
    "    return trimmed_dfs\n",
    "\n",
    "# Apply the cleaning process to all DataFrames\n",
    "trimmed_all_dfs = double_quotes_and_trim_dataframes(dataframes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=dataframes_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=dataframes_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# cols=df1.select_dtypes(['object']).columns\n",
    "# df1[cols]=df1[cols].apply(lambda x: x.str.replace('\"','').str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of your 4 DataFrames\n",
    "dfs = [df1, df2]\n",
    "\n",
    "# Define the exact target column order and names\n",
    "target_columns = [\n",
    "    'DTT_ID', 'Profit Centre', 'Engagement Partner', 'Engagement Manager',\n",
    "       'Client Code', 'Client Name', 'WBS Level 2', 'Engagement Name',\n",
    "       'Engagement Type', 'Contract Type', 'Total Hours',\n",
    "       'Gross Services Revenue', 'Gross Revenue ADM',\n",
    "       'Engagement Service Revenue', 'Event / Success Achieved Revenue',\n",
    "       'Expenses Over / Under Recovery Revenue',\n",
    "       'Unplanned Revenue Adjustments', 'Net Services Revenue', 'ADM Revenue',\n",
    "       'Product Revenue', 'Asset Revenue', 'Other Revenue', 'Net Revenue',\n",
    "       'Total Cost', 'Client Margin', 'Client Margin %', 'Net Rev/Hour',\n",
    "       'Standard Recovery %', 'Adjusted Net Revenue',\n",
    "       'Adjusted Client Margin %', 'Financial Year', 'Month', 'Period Type',\n",
    "       'DTT_FILENAME'\n",
    "]\n",
    "\n",
    "# Standardize column names and align order\n",
    "for i in range(len(dfs)):\n",
    "    df = dfs[i]\n",
    "\n",
    "    # Rename 'Year' to 'Financial Year' if present\n",
    "    if 'Year' in df.columns:\n",
    "        df = df.rename(columns={'Year': 'Financial Year'})\n",
    "\n",
    "    # Reorder columns to match target list\n",
    "    df = df[[col for col in target_columns if col in df.columns]]\n",
    "\n",
    "    dfs[i] = df\n",
    "\n",
    "# Combine all aligned DataFrames\n",
    "df_combined_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Final alignment to ensure column sequence\n",
    "df_combined_all = df_combined_all[target_columns]\n",
    "\n",
    "# Optional: quick preview\n",
    "display(df_combined_all.head(2))\n",
    "print(f\"Total combined rows: {len(df_combined_all)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Read spotlight configure files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import os\n",
    "# import glob\n",
    "\n",
    "# log_directory = r''\n",
    "\n",
    "# log_file_pattern = os.path.join(log_directory, '*.splog')\n",
    "\n",
    "# def calculate_row_count(log_file_path):\n",
    "#     total_row_count = 0\n",
    "#     row_count_pattern = re.compile(r'Returned (\\d+) rows from query JOURNALS')\n",
    "\n",
    "#     with open(log_file_path, 'r') as log_file:\n",
    "#         for line in log_file:\n",
    "#             match = row_count_pattern.search(line)\n",
    "#             if match:\n",
    "#                 row_count = int(match.group(1))\n",
    "#                 total_row_count += row_count\n",
    "\n",
    "#     return total_row_count\n",
    "\n",
    "# row_counts_per_file = {}\n",
    "\n",
    "# log_files = glob.glob(log_file_pattern)\n",
    "\n",
    "# for log_file_path in log_files:\n",
    "#     row_count = calculate_row_count(log_file_path)\n",
    "#     row_counts_per_file[log_file_path] = row_count\n",
    "\n",
    "# total_row_count_all_files = sum(row_counts_per_file.values())\n",
    "\n",
    "# print('Row count per log file:')\n",
    "# for log_file_path, row_count in row_counts_per_file.items():\n",
    "#     print(f'{log_file_path}: {row_count}')\n",
    "\n",
    "# print(f'\\nTotal row count across all log files: {total_row_count_all_files}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xml.etree.ElementTree as ET\n",
    "# import html\n",
    "\n",
    "# # Define the file path\n",
    "# file_path = r''\n",
    "\n",
    "# # Parse the XML file\n",
    "# tree = ET.parse(file_path)\n",
    "# root = tree.getroot()\n",
    "\n",
    "# # Function to decode and parse the embedded query XML\n",
    "# def decode_and_print_query_xml(encoded_query_xml):\n",
    "#     # Decode the HTML entities in the XML string\n",
    "#     decoded_xml = html.unescape(encoded_query_xml)\n",
    "    \n",
    "#     # Parse the decoded XML string\n",
    "#     query_tree = ET.ElementTree(ET.fromstring(decoded_xml))\n",
    "#     query_root = query_tree.getroot()\n",
    "    \n",
    "#     # Print the parsed query details\n",
    "#     for child in query_root:\n",
    "#         print(f\"{child.tag}: {child.text}\")\n",
    "#         if len(child):\n",
    "#             for subchild in child:\n",
    "#                 print(f\"  {subchild.tag}: {subchild.text}\")\n",
    "#                 if len(subchild):\n",
    "#                     for subsubchild in subchild:\n",
    "#                         print(f\"    {subsubchild.tag}: {subsubchild.text}\")\n",
    "#     print(\"---\")\n",
    "\n",
    "# # Find the 'Query' elements and process the 'msprop:AdvancedQueryXML' attribute\n",
    "# for query in root.findall('.//xs:element[@name=\"Query\"]', namespaces={'xs': 'http://www.w3.org/2001/XMLSchema'}):\n",
    "#     advanced_query_xml = query.get('{urn:schemas-microsoft-com:xml-msprop}AdvancedQueryXML')\n",
    "#     if advanced_query_xml:\n",
    "#         decode_and_print_query_xml(advanced_query_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xml.etree.ElementTree as ET\n",
    "\n",
    "# # Define the file path\n",
    "# file_path = r''\n",
    "\n",
    "# # Parse the XML file\n",
    "# tree = ET.parse(file_path)\n",
    "# root = tree.getroot()\n",
    "\n",
    "# # Function to print the content of a query element in a readable format\n",
    "# def print_query(query):\n",
    "#     name = query.find('Name').text if query.find('Name') is not None else 'N/A'\n",
    "#     field_name = query.find('Field_x0020_Name').text if query.find('Field_x0020_Name') is not None else 'N/A'\n",
    "#     description = query.find('Description').text if query.find('Description') is not None else 'N/A'\n",
    "    \n",
    "#     print(f\"Name: {name}\")\n",
    "#     print(f\"Field Name: {field_name}\")\n",
    "#     print(f\"Description: {description}\")\n",
    "#     print(\"---\")\n",
    "\n",
    "# # Iterate through the XML tree and print each Query element\n",
    "# for query in root.findall('Query'):\n",
    "#     print_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Read txt/csv in one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "# import csv\n",
    "\n",
    "# def read_and_combine_files(master_path):\n",
    "#     # Find both .txt and .csv files recursively\n",
    "#     all_data_files = list(master_path.rglob('*.txt')) + list(master_path.rglob('Hierachy JAN.csv'))\n",
    "#     combined_dfs = {}\n",
    "\n",
    "#     for file in all_data_files:\n",
    "#         try:\n",
    "#             # Determine delimiter based on file extension\n",
    "#             delimiter = '\\t' if file.suffix.lower() == '.txt' else ','\n",
    "            \n",
    "#             df = pd.read_csv(file,\n",
    "#                              encoding='utf-8',\n",
    "#                              index_col=False,\n",
    "#                              header=0,\n",
    "#                              dtype=str,\n",
    "#                              delimiter=delimiter,\n",
    "#                              quoting=csv.QUOTE_MINIMAL)\n",
    "            \n",
    "#             # Insert ID and filename\n",
    "#             if 'DTT_ID' not in df.columns:\n",
    "#                 df.insert(0, 'DTT_ID', range(1, len(df) + 1))\n",
    "\n",
    "#             df['DTT_FILENAME'] = file.name\n",
    "            \n",
    "#             columns_tuple = tuple(df.columns)\n",
    "#             if columns_tuple in combined_dfs:\n",
    "#                 combined_dfs[columns_tuple] = pd.concat([combined_dfs[columns_tuple], df], ignore_index=True)\n",
    "#             else:\n",
    "#                 combined_dfs[columns_tuple] = df\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing file {file.name}: {e}\")\n",
    "\n",
    "#     return combined_dfs\n",
    "\n",
    "# # Usage\n",
    "# master_folder_path = Path(r'C:\\Users\\ele\\OneDrive - Deloitte (O365D)\\Projects\\IB_Asset\\Operate Revenue Reporting\\12.MAY\\PC')\n",
    "# combined_dataframes = read_and_combine_files(master_folder_path)\n",
    "\n",
    "# # Display previews\n",
    "# for i, df in enumerate(combined_dataframes.values()):\n",
    "#     print(f\"DataFrame {i+1} Preview:\")\n",
    "#     display(df.head(2))\n",
    "#     print(f\"Total rows in DataFrame {i+1}: {len(df)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Read SAS format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import pyreadstat\n",
    "# import pandas as pd\n",
    "\n",
    "# def read_and_combine_csv(master_path):\n",
    "#     all_sas_files = master_path.rglob('*.sas7bdat')  # Recursively find all SAS files\n",
    "#     combined_dfs = {}  # Dictionary to store combined DataFrames\n",
    "\n",
    "#     for sas_file in all_sas_files:\n",
    "#         # Read the SAS file with pyreadstat\n",
    "#         df, meta = pyreadstat.read_sas7bdat(sas_file)\n",
    "\n",
    "#         # Convert all values to strings in a way that ensures no floating point transformation\n",
    "#         df = df.applymap(lambda x: '{:.0f}'.format(x) if isinstance(x, float) and x.is_integer() else str(x))\n",
    "\n",
    "#         # Add ID column at the beginning\n",
    "#         df.insert(0, 'DTT_ID', range(1, len(df) + 1))\n",
    "\n",
    "#         # Add Filename column at the end\n",
    "#         df['DTT_FILENAME'] = sas_file.name\n",
    "\n",
    "#         columns_tuple = tuple(df.columns)  # Tuple of column names for comparison\n",
    "\n",
    "#         if columns_tuple in combined_dfs:\n",
    "#             # Append to the existing DataFrame with the same columns\n",
    "#             combined_dfs[columns_tuple] = pd.concat([combined_dfs[columns_tuple], df], ignore_index=True)\n",
    "#         else:\n",
    "#             # Create a new entry in the dictionary for these columns\n",
    "#             combined_dfs[columns_tuple] = df\n",
    "\n",
    "#     return combined_dfs\n",
    "\n",
    "# # Usage\n",
    "# master_folder_path = Path(r'C:\\Users\\ele\\Downloads\\20241016 Data request - Service Item Audit (1)')\n",
    "# combined_dataframes = read_and_combine_csv(master_folder_path)\n",
    "\n",
    "# # Displaying combined dataframes\n",
    "# for i, df in enumerate(combined_dataframes.values()):\n",
    "#     print(f\"DataFrame {i+1} Preview:\")\n",
    "#     display(df.head(5))\n",
    "#     print(f\"Total rows in DataFrame {i+1}: {len(df)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Read parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "\n",
    "# def read_and_combine_parquet(master_path):\n",
    "#     all_parquet_files = master_path.rglob('*.parquet')  # Recursively find all Parquet files\n",
    "#     combined_dfs = {}  # Dictionary to store combined DataFrames\n",
    "\n",
    "#     for parquet_file in all_parquet_files:\n",
    "#         df = pd.read_parquet(parquet_file, engine='pyarrow')\n",
    "#         df = df.astype(str)\n",
    "        \n",
    "#         # Add ID column at the beginning\n",
    "#         df.insert(0, 'DTT_ID', range(1, len(df) + 1))\n",
    "\n",
    "#         # Add Filename column at the end\n",
    "#         df['DTT_FILENAME'] = parquet_file.name\n",
    "\n",
    "#         columns_tuple = tuple(df.columns)  # Tuple of column names for comparison\n",
    "\n",
    "#         if columns_tuple in combined_dfs:\n",
    "#             # Append to the existing DataFrame with the same columns\n",
    "#             combined_dfs[columns_tuple] = pd.concat([combined_dfs[columns_tuple], df], ignore_index=True)\n",
    "#         else:\n",
    "#             # Create a new entry in the dictionary for these columns\n",
    "#             combined_dfs[columns_tuple] = df\n",
    "\n",
    "#     return combined_dfs\n",
    "\n",
    "# # Usage\n",
    "# master_folder_path = Path(r'C:\\Users\\ele\\Downloads')\n",
    "# combined_dataframes = read_and_combine_parquet(master_folder_path)\n",
    "\n",
    "# # Displaying combined dataframes\n",
    "# for i, df in enumerate(combined_dataframes.values()):\n",
    "#     print(f\"DataFrame {i+1} Preview:\")\n",
    "#     display(df.head(5))\n",
    "#     print(f\"Total rows in DataFrame {i+1}: {len(df)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Read fixedwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "\n",
    "# def read_and_combine_fwf(master_path, colspecs): #, colnames\n",
    "#     all_fwf_files = master_path.rglob('*.csv')  # Replace with the correct extension for your files\n",
    "#     combined_dfs = {}  # Dictionary to store combined DataFrames\n",
    "\n",
    "#     for fwf_file in all_fwf_files:\n",
    "#         df = pd.read_fwf(fwf_file, colspecs=colspecs, header=0, dtype=str) #, names=colnames\n",
    "\n",
    "#         # Add ID column at the beginning\n",
    "#         df.insert(0, 'DTT_ID', range(1, len(df) + 1))\n",
    "\n",
    "#         # Add Filename column at the end\n",
    "#         df['DTT_FILENAME'] = str(fwf_file.absolute())\n",
    "\n",
    "#         columns_tuple = tuple(df.columns)  # Tuple of column names for comparison\n",
    "\n",
    "#         if columns_tuple in combined_dfs:\n",
    "#             # Append to the existing DataFrame with the same columns\n",
    "#             combined_dfs[columns_tuple] = pd.concat([combined_dfs[columns_tuple], df], ignore_index=True)\n",
    "#         else:\n",
    "#             # Create a new entry in the dictionary for these columns\n",
    "#             combined_dfs[columns_tuple] = df\n",
    "\n",
    "#     return combined_dfs\n",
    "\n",
    "# # Column specifications for your fixed-width files\n",
    "# colspecs = [(0, 16), (16, 56), (56, 82), (82, 122), (122, 162), (162, 173), (173, 187), (187, 200), (200, 213), (213, 224), (224, 245), (245, 285), (285, 297), (297, 337), (337, 377), (377, 386), (386, 426), (426, 439), (439, 452), (452, 463), (463, 476),(476,1000)]\n",
    "# #colnames = ['JournalEntryID', 'JounalLineNumber', 'JournalEntryDescription', 'FiscalPeriod', 'FiscalYear', 'PostedDate', 'EffectiveDate', 'UserID', 'StdIndicator', 'GLAcct', 'GLAcctName', 'TransactionAmount', 'CDIndicator', 'DebitAmount', 'CreditAmount', 'Currency', 'LocalAmount', 'SourceSystem', 'DocumentType', 'EntryDate', 'DocumentDate', 'ProfitCenter']\n",
    "\n",
    "# # Usage\n",
    "# master_folder_path = Path(r'')\n",
    "# combined_dataframes = read_and_combine_fwf(master_folder_path, colspecs) #, colnames\n",
    "\n",
    "# # Displaying combined dataframes\n",
    "# for i, df in enumerate(combined_dataframes.values()):\n",
    "#     print(f\"DataFrame {i+1} Preview:\")\n",
    "#     display(df.head(5))\n",
    "#     print(f\"Total rows in DataFrame {i+1}: {len(df)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Convert the dictionary values to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes_list = list(combined_dataframes.values())\n",
    "\n",
    "# # Access the second DataFrame (remember, Python uses 0-based indexing)\n",
    "# #second_dataframe = dataframes_list[1]\n",
    "\n",
    "# # Now you can work with second_dataframe\n",
    "# print(\"DataFrame Preview:\")\n",
    "# display(dataframes_list[0].head(2))\n",
    "# print(f\"Total rows in Second DataFrame: {len(dataframes_list[0])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2=dataframes_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Rename using one of the rows to replace column header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def replace_headers_with_second_row(df):\n",
    "#     # Ensure DataFrame has at least two rows\n",
    "#     if len(df) < 2:\n",
    "#         raise ValueError(\"DataFrame must have at least two rows to use the second row as headers\")\n",
    "\n",
    "#     # Set the second row as the new header for middle columns\n",
    "#     middle_columns = df.columns[1:-1]  # Select all columns except the first and last\n",
    "#     new_header = df.iloc[1, 1:-1]  # Grab the second row for the middle columns header\n",
    "#     df.iloc[1:, 1:-1].columns = new_header  # Set the new headers for the middle columns\n",
    "\n",
    "#     # Keep the first and last column headers unchanged\n",
    "#     df.columns = [df.columns[0]] + list(new_header) + [df.columns[-1]]\n",
    "\n",
    "#     # Remove the first two rows (original header and the row used as new header)\n",
    "#     df = df[2:]\n",
    "\n",
    "#     # Reset the index of the DataFrame\n",
    "#     df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes_list[1]=replace_headers_with_second_row(dataframes_list[1])\n",
    "\n",
    "# print(dataframes_list[1].shape)\n",
    "\n",
    "# dataframes_list[1].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trimmed_all_dfs[0]['DTT_FILENAME'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trimmed_all_dfs[0] = trimmed_all_dfs[0].rename(columns={\n",
    "#     '√Ø¬ª¬øDataID': 'DataID'\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trimmed_all_dfs[0]=trimmed_all_dfs[0][trimmed_all_dfs[0]['DataID'].notna()]\n",
    "# print(trimmed_all_dfs[0].shape)\n",
    "# trimmed_all_dfs[0].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# union_df = pd.concat([df_combined, df_2], ignore_index=True)\n",
    "\n",
    "# print(union_df.shape)\n",
    "# union_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trimmed_all_dfs[0]=union_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Change column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename_dict = {\n",
    "#     '√Ø¬ª¬øProfit Centre': 'Profit Centre',\n",
    "# #    'OldName2': 'NewName2',\n",
    "#     # Add more columns as needed\n",
    "# }\n",
    "\n",
    "# for df in trimmed_all_dfs:\n",
    "#     df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "# trimmed_all_dfs[0].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    If column name follows a certain pattern, can use following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def rename_columns_by_pattern(df, pattern, replacement):\n",
    "#     new_column_names = {col: re.sub(pattern, replacement, col) for col in df.columns}\n",
    "#     df.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "# pattern = r'Old'  # Regex pattern to match in the column names\n",
    "# replacement = 'New'  # Replacement string\n",
    "\n",
    "# for df in trimed_all_dfs:\n",
    "#     rename_columns_by_pattern(df, pattern, replacement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyodbc\n",
    "# from datetime import datetime\n",
    "# from sqlalchemy import create_engine\n",
    "\n",
    "# # Server and database configuration\n",
    "# server = ''\n",
    "# database = ''\n",
    "# connection_string = f\"mssql+pyodbc://@{server}/{database}?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes\"\n",
    "\n",
    "# # Create SQL Alchemy engine\n",
    "# engine = create_engine(connection_string, fast_executemany=True)\n",
    "\n",
    "# def upload_dataframe_to_sql(df, table_name, initials):\n",
    "#     # Define the full table name with date and initials\n",
    "#     today = datetime.now().strftime('%Y%m%d')\n",
    "#     full_table_name = f'{table_name}_{today}_{initials}'\n",
    "\n",
    "#     # Upload the DataFrame to SQL Server\n",
    "#     df.to_sql(full_table_name, engine, if_exists='replace', index=False, chunksize=10000)\n",
    "\n",
    "#     # Return the number of rows uploaded and the full table name\n",
    "#     return len(df), full_table_name\n",
    "\n",
    "\n",
    "# # Table name mapping\n",
    "# table_name_mapping = {\n",
    "#     0: 'QA_TEST_CASE',\n",
    "\n",
    "#     # Add mappings as needed\n",
    "#     # i: 'TableName' i is index of each dataframe\n",
    "# }\n",
    "\n",
    "# initials = \"EL\"  # Replace with the desired initials\n",
    "\n",
    "# # Specify the index from which to start uploading\n",
    "# start_from_index = 0  # Change this to start from a different DataFrame\n",
    "# end_from_index = 1   # Change this to end at a different DataFrame\n",
    "\n",
    "# # Loop through trimmed_all_dfs and upload each DataFrame within the specified range\n",
    "# for i, df in enumerate(trimmed_all_dfs[start_from_index:end_from_index], start=start_from_index):\n",
    "#     base_table_name = table_name_mapping.get(i, f'{i}')  # Default name if not in mapping\n",
    "#     row_count, full_table_name = upload_dataframe_to_sql(df, base_table_name, initials)\n",
    "#     print(f\"Uploaded {row_count} rows to table {full_table_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pyodbc\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Server and database configuration\n",
    "server = ''\n",
    "database = ''\n",
    "connection_string = f\"mssql+pyodbc://@{server}/{database}?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes\"\n",
    "\n",
    "# Create SQL Alchemy engine\n",
    "engine = create_engine(connection_string, fast_executemany=True)\n",
    "\n",
    "def _validate_schema_name(schema: str) -> str:\n",
    "    if schema is None:\n",
    "        return None\n",
    "    if re.fullmatch(r\"[A-Za-z_][A-Za-z0-9_]*\", schema):\n",
    "        return schema\n",
    "    raise ValueError(\"Invalid schema name. Use letters, numbers, and underscores; cannot start with a number.\")\n",
    "\n",
    "def ensure_schema_exists(schema: str):\n",
    "    \"\"\"Create schema if it doesn't exist (SQL Server).\"\"\"\n",
    "    if not schema:\n",
    "        return\n",
    "    safe_schema = _validate_schema_name(schema)\n",
    "    tsql = f\"\"\"\n",
    "IF NOT EXISTS (SELECT 1 FROM sys.schemas WHERE name = '{safe_schema}')\n",
    "    EXEC('CREATE SCHEMA [{safe_schema}] AUTHORIZATION [dbo]');\n",
    "\"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        conn.exec_driver_sql(tsql)\n",
    "\n",
    "def upload_dataframe_to_sql(df, table_name, initials, schema: str = None, prefix: str = \"RAW\"):\n",
    "    # Validate DataFrame columns\n",
    "    if df is None:\n",
    "        raise ValueError(\"DataFrame is None.\")\n",
    "    if df.columns.hasnans or df.columns.duplicated().any():\n",
    "        raise ValueError(\"DataFrame contains columns with None or duplicate names.\")\n",
    "\n",
    "    # Define the full table name with date and initials (unqualified; schema handled separately)\n",
    "    today = datetime.now().strftime('%Y%m%d')\n",
    "    base_table_name = f'{prefix}_{table_name}_{today}_{initials}'\n",
    "\n",
    "    # Ensure target schema exists (if provided)\n",
    "    ensure_schema_exists(schema)\n",
    "\n",
    "    # Upload the DataFrame to SQL Server\n",
    "    with engine.begin() as conn:\n",
    "        df.to_sql(\n",
    "            base_table_name,\n",
    "            conn,\n",
    "            schema=schema,  # <- write into desired schema (e.g., INPUT/OUTPUT/ANALYSIS)\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            chunksize=10000\n",
    "        )\n",
    "\n",
    "    qualified_name = f\"{schema}.{base_table_name}\" if schema else base_table_name\n",
    "    return len(df), qualified_name\n",
    "\n",
    "initials = \"EL\"  # Replace with the desired initials\n",
    "\n",
    "# Define the base table name\n",
    "base_table_name = 'REVENUE'  # Replace with your desired base table name or use a mapped name\n",
    "\n",
    "# Choose the target schema (e.g., 'INPUT', 'OUTPUT', 'ANALYSIS'); set to None to use dbo/default\n",
    "target_schema = 'INPUT'\n",
    "\n",
    "# Upload the DataFrame to SQL Server\n",
    "if 'df_combined_all' in globals() and df_combined_all is not None:\n",
    "    try:\n",
    "        row_count, full_table_name = upload_dataframe_to_sql(df_combined_all, base_table_name, initials, schema=target_schema)\n",
    "        print(f\"Uploaded {row_count} rows to table {full_table_name}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"The DataFrame is None and was not uploaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
