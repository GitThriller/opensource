{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:pink;\">üìä Excel Data ETL Script</h1>\n",
    "<h3 style=\"color:purple;\">This script involves: </h3>\n",
    "\n",
    "        üîç Detecting & Handling Excel File Formats\n",
    "        üîê Decrypting Encrypted Files\n",
    "        üìê Analyzing Workbook Structures\n",
    "        üîÑ Consolidating Data into DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Viewing Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import msoffcrypto\n",
    "import tempfile\n",
    "import csv\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#pd.set_option('display.width', None)\n",
    "#np.set_printoptions(threshold=np.inf)\n",
    "#pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read excel\n",
    "- with multiple workbook \n",
    "- with multiple worksheets \n",
    "- with consistent column format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_encrypted(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            office_file = msoffcrypto.OfficeFile(f)\n",
    "            return office_file.is_encrypted()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Encryption check failed: {file_path} ‚Äì {e}\")\n",
    "        return False\n",
    "\n",
    "def decrypt_excel(file_path, password):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            office_file = msoffcrypto.OfficeFile(f)\n",
    "            office_file.load_key(password=password)\n",
    "            with tempfile.NamedTemporaryFile(suffix=file_path.suffix, delete=False) as tmp:\n",
    "                office_file.decrypt(tmp)\n",
    "                return Path(tmp.name)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Decryption failed for {file_path}: {e}\")\n",
    "\n",
    "def load_and_combine_excels(master_path, password=\"\"):\n",
    "    all_dataframes = []\n",
    "    excel_suffixes = ['.xlsx', '.xls', '.xlsb']\n",
    "\n",
    "    for path in Path(master_path).rglob(\"*ATO lodgement confirmation*\"):\n",
    "        if path.suffix.lower() not in excel_suffixes:\n",
    "            continue\n",
    "\n",
    "        original_path = path  # ‚úÖ Store original full path\n",
    "        file_path = path\n",
    "\n",
    "        print(f\"üìÑ Found: {original_path}\")\n",
    "\n",
    "        if is_encrypted(file_path):\n",
    "            if not password:\n",
    "                print(f\"üîí Skipping encrypted file (no password): {file_path}\")\n",
    "                continue\n",
    "            try:\n",
    "                print(f\"üîì Decrypting: {file_path}\")\n",
    "                file_path = decrypt_excel(file_path, password)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå {e}\")\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            workbook = pd.ExcelFile(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to open {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for sheet in workbook.sheet_names:\n",
    "            try:\n",
    "                df = pd.read_excel(workbook, sheet_name=sheet, dtype=str, header=0)#engine='openpyxl',\n",
    "                df['DTT_FILENAME'] = str(file_path)  # Might be temp path\n",
    "                df['DTT_FILENAME_ORIGINAL'] = str(original_path)  # ‚úÖ Actual user file\n",
    "                df['DTT_SHEETNAME'] = sheet\n",
    "                df['DTT_ID'] = range(1, len(df) + 1)\n",
    "                all_dataframes.append(df)\n",
    "                print(f\"‚úÖ Loaded {sheet} ({len(df)} rows)\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to read sheet '{sheet}' in {file_path}: {e}\")\n",
    "\n",
    "    if not all_dataframes:\n",
    "        print(\"‚ö†Ô∏è No data loaded.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Combine all data (optionally align columns)\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "    print(f\"\\nüéâ Combined total rows: {len(combined_df)} from {len(all_dataframes)} sheets\")\n",
    "    return combined_df\n",
    "\n",
    "# üöÄ Example usage\n",
    "master_path = r''\n",
    "password = \"\"  # replace with actual password or leave \"\" for unencrypted only\n",
    "\n",
    "final_df = load_and_combine_excels(master_path, password)\n",
    "\n",
    "# final_df = final_df[final_df['Benefit ID'].str.strip().str.lower() != 'grand total']\n",
    "\n",
    "# Display a preview\n",
    "print('üéâ Combined total rows after grand total line removal: ',final_df.shape,'\\n')\n",
    "print(\"\\nüîç Preview of combined DataFrame:\")\n",
    "final_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove trailing spaces and double quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=final_df.select_dtypes(['object']).columns\n",
    "final_df[cols]=final_df[cols].apply(lambda x: x.str.replace('\\n','').str.replace('\"','').str.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort final_df by the column 'Title'\n",
    "sorted_df_1 = final_df.sort_values(by='Title', ascending=True)\n",
    "\n",
    "# Display a preview of the sorted DataFrame\n",
    "print(\"\\nüîç Preview of sorted DataFrame:\")\n",
    "sorted_df_1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort final_df by the column 'Title'\n",
    "sorted_df_2 = final_df.sort_values(by='Entity Name', ascending=True)\n",
    "\n",
    "# Display a preview of the sorted DataFrame\n",
    "print(\"\\nüîç Preview of sorted DataFrame:\")\n",
    "sorted_df_2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort final_df by the column 'Title'\n",
    "sorted_df_3 = final_df.sort_values(by='EntityName', ascending=True)\n",
    "\n",
    "# Display a preview of the sorted DataFrame\n",
    "print(\"\\nüîç Preview of sorted DataFrame:\")\n",
    "sorted_df_3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_column_differences():\n",
    "    mismatched_df = []\n",
    "    for col1 in sorted_df_1.columns:\n",
    "        for col2 in sorted_df_2.columns:\n",
    "            if col1 != col2:\n",
    "                mismatched = sorted_df_1[~sorted_df_1[col1].isin(sorted_df_2[col2])]\n",
    "                if not mismatched.empty:\n",
    "                    mismatched['Source Column'] = col1\n",
    "                    mismatched['Target Column'] = col2\n",
    "                    mismatched_df.append(mismatched)\n",
    "    if mismatched_df:\n",
    "        print(f\"‚ö†Ô∏è Found {len(mismatched_df)} mismatched columns between sorted_df_1 and sorted_df_2\")\n",
    "\n",
    "identify_column_differences() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_missing_entities():\n",
    "    mismatched_df = []\n",
    "    for value in sorted_df_3['EntityName'].unique():\n",
    "        if value not in sorted_df_2['Entity Name'].values:\n",
    "            mismatched_df.append(value)\n",
    "    if mismatched_df:\n",
    "        print(f\"‚ö†Ô∏è Found {len(mismatched_df)} mismatched values in 'Title' not found in 'Entity Name': {mismatched_df}\")\n",
    "    else:\n",
    "        print(\"‚úÖ All values in 'Title' exist in 'Entity Name'.\")\n",
    "\n",
    "report_missing_entities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def map_columns(df):\n",
    "    # Extract code and name from filename like \"BenefitComparison - 1043 (CNSCV).xlsx\"\n",
    "    def extract_code_name(path):\n",
    "        match = re.search(r'(\\d+)\\s*\\(([^)]+)\\)\\.xlsx$', str(path))\n",
    "        if match:\n",
    "            return match.group(1), match.group(2)\n",
    "        return None, None\n",
    "\n",
    "    df['Reporting Entity Code'], df['Reporting Entity Name'] = zip(*df['DTT_FILENAME_ORIGINAL'].map(extract_code_name))\n",
    "\n",
    "    # Add constant column\n",
    "    df['FBT Year'] = \n",
    "\n",
    "    # Remote area logic\n",
    "    # df['Remote area'] = df['Business Amount.1'].fillna('0').apply(\n",
    "    #     lambda x: 'Non-remote' if str(x).strip() in ['0', '0.0', '', 'nan', '$0.00'] else 'Remote'\n",
    "    # )\n",
    "\n",
    "    # üí£ Drop pre-existing base column names to avoid rename duplication\n",
    "    # df = df.drop(columns=[\n",
    "    #     col for col in ['Total Taxable Value', 'Gross Taxable Value', 'Business Amount']\n",
    "    #     if col in df.columns\n",
    "    # ])\n",
    "\n",
    "    # Rename columns\n",
    "    df = df.rename(columns={\n",
    "        'Benefit ID': 'Benefit ID',\n",
    "    })\n",
    "\n",
    "    # Final output ordering\n",
    "    final_cols = [\n",
    "        'Benefit ID',\n",
    "    ]\n",
    "\n",
    "    return df[final_cols]\n",
    "\n",
    "\n",
    "# üß† Apply the mapping\n",
    "mapped_df = map_columns(final_df)\n",
    "\n",
    "# üîç Preview\n",
    "print(\"\\nüéØ Mapped DataFrame:\")\n",
    "print(mapped_df.shape,'\\n')\n",
    "mapped_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from raw names to cleaned full names\n",
    "entity_name_map = {\n",
    "    'CNSCV': 'CNSCV',\n",
    "    'CNSCE': 'CNSCE Pty Ltd',\n",
    "\n",
    "}\n",
    "\n",
    "# Apply mapping\n",
    "mapped_df['Reporting Entity Name'] = mapped_df['Reporting Entity Name'].str.strip().map(entity_name_map).fillna(mapped_df['Reporting Entity Name'])\n",
    "\n",
    "mapped_df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define local file path\n",
    "local_output_file = r\"\"\n",
    "\n",
    "# Save the DataFrame to an Excel file locally\n",
    "mapped_df.to_excel(local_output_file, index=False, sheet_name=\"\")\n",
    "\n",
    "print(f\"File saved at: {local_output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyodbc\n",
    "# from datetime import datetime\n",
    "# from sqlalchemy import create_engine\n",
    "\n",
    "# # Server and database configuration\n",
    "# server = ''\n",
    "# database = ''\n",
    "# connection_string = f\"mssql+pyodbc://@{server}/{database}?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes\"\n",
    "\n",
    "# # Create SQL Alchemy engine\n",
    "# engine = create_engine(connection_string, fast_executemany=True)\n",
    "\n",
    "# def upload_dataframe_to_sql(df, table_name, initials):\n",
    "#     # Define the full table name with date and initials\n",
    "#     today = datetime.now().strftime('%Y%m%d')\n",
    "#     full_table_name = f'RAW_{table_name}_{today}_{initials}'\n",
    "\n",
    "#     # Upload the DataFrame to SQL Server\n",
    "#     df.to_sql(full_table_name, engine, if_exists='replace', index=False, chunksize=10000)\n",
    "\n",
    "#     # Return the number of rows uploaded and the full table name\n",
    "#     return len(df), full_table_name\n",
    "\n",
    "# # Table name mapping\n",
    "# table_name_mapping = {\n",
    "#     0: 'SPOTLESS_CONCUR',\n",
    "# #    1: 'SPOTLESS_GL_450580',\n",
    "#     # Add mappings as needed\n",
    "#     # i: 'TableName' i is index of each dataframe\n",
    "# }\n",
    "\n",
    "# initials = \"EL\"  # Replace with the desired initials\n",
    "\n",
    "# # Specify the index from which to start uploading\n",
    "# start_from_index = 0  # Change this to start from a different DataFrame\n",
    "# end_from_index = 0   # Change this to end at a different DataFrame\n",
    "\n",
    "# # Loop through trimmed_all_dfs and upload each DataFrame within the specified range\n",
    "# for i, df in enumerate(trimmed_all_dfs):\n",
    "#     base_table_name = table_name_mapping.get(i, f'{i+1}')  # Default name if not in mapping\n",
    "#     row_count, full_table_name = upload_dataframe_to_sql(df, base_table_name, initials)\n",
    "#     print(f\"Uploaded {row_count} rows to table {full_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyodbc\n",
    "# from datetime import datetime\n",
    "# from sqlalchemy import create_engine\n",
    "\n",
    "# # Server and database configuration\n",
    "# server = ''\n",
    "# database = ''\n",
    "# connection_string = f\"mssql+pyodbc://@{server}/{database}?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes\"\n",
    "\n",
    "# # Create SQL Alchemy engine\n",
    "# engine = create_engine(connection_string, fast_executemany=True)\n",
    "\n",
    "# def upload_dataframe_to_sql(df, table_name, initials):\n",
    "#     # Check for None or duplicate column names\n",
    "#     if df.columns.hasnans or df.columns.duplicated().any():\n",
    "#         raise ValueError(\"DataFrame contains columns with None or duplicate names.\")\n",
    "\n",
    "#     # Define the full table name with date and initials\n",
    "#     today = datetime.now().strftime('%Y%m%d')\n",
    "#     full_table_name = f'RAW_{table_name}_{today}_{initials}'\n",
    "\n",
    "#     # Upload the DataFrame to SQL Server\n",
    "#     df.to_sql(full_table_name, engine, if_exists='replace', index=False, chunksize=10000)\n",
    "\n",
    "#     # Return the number of rows uploaded and the full table name\n",
    "#     return len(df), full_table_name\n",
    "\n",
    "# initials = \"EL\"  # Replace with the desired initials\n",
    "\n",
    "# # Define the base table name\n",
    "# base_table_name = 'Hierarchy_Changes'  # Replace with your desired base table name or use a mapped name\n",
    "\n",
    "# # Upload the DataFrame to SQL Server\n",
    "# if df is not None:\n",
    "#     try:\n",
    "#         row_count, full_table_name = upload_dataframe_to_sql(df, base_table_name, initials)\n",
    "#         print(f\"Uploaded {row_count} rows to table {full_table_name}\")\n",
    "#     except ValueError as e:\n",
    "#         print(f\"Error: {e}\")\n",
    "# else:\n",
    "#     print(\"The DataFrame is None and was not uploaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read excel\n",
    "- Inconsistent format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##    Part 1:\n",
    "\n",
    "        - Search for Excel files in all subfolders of the specified master_path\n",
    "        - Categorize these files by their extension (xlsx, xls, xlsb)\n",
    "        - Print the count and the paths of the files in each category\n",
    "        - Return a dictionary where each key is a file extension and the value is a list of paths to files with that extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# from glob import glob\n",
    "\n",
    "\n",
    "# def detect_excel_files(master_path):\n",
    "#     excel_files = {'xlsx': [], 'xls': [], 'xlsb': []}\n",
    "\n",
    "#     for file_path in master_path.rglob('*.*'):\n",
    "#         if file_path.suffix.lower() in ['.xlsx', '.xls', '.xlsb']:\n",
    "#             excel_files[file_path.suffix.lower()[1:]].append(file_path)\n",
    "\n",
    "#     for key in excel_files:\n",
    "#         print(f\"{len(excel_files[key])}.{key} files found:\")\n",
    "#         for file in excel_files[key]:\n",
    "#             print(f\"  - {file}\")\n",
    "\n",
    "#     return excel_files\n",
    "\n",
    "# # Define the master path\n",
    "# master_path=Path(r'C:\\Users\\ele\\Downloads\\New folder')\n",
    "# #raw_path=master_path/'1.Original'\n",
    "# #input_path=master_path/'2.Precleaned'\n",
    "# #output_path=master_path/'3.Cleaned'\n",
    "# excel_files = detect_excel_files(master_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##    Part 2:\n",
    "\n",
    "        - is_encrypted checks if a file is encrypted\n",
    "        - decrypt_excel handles the decryption of an encrypted file\n",
    "        - check_and_decrypt_files iterates over all detected Excel files, checks if they are encrypted, and decrypts them if necessary\n",
    "        It prints the status of each file and stores the paths to the decrypted files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import msoffcrypto\n",
    "# import tempfile\n",
    "\n",
    "# def is_encrypted(file_path):\n",
    "#     try:\n",
    "#         with open(file_path, \"rb\") as file:\n",
    "#             office_file = msoffcrypto.OfficeFile(file)\n",
    "#             return office_file.is_encrypted()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error checking if file is encrypted: {e}\")\n",
    "#         return False\n",
    "\n",
    "# def decrypt_excel(file_path, password):\n",
    "#     with open(file_path, \"rb\") as file:\n",
    "#         office_file = msoffcrypto.OfficeFile(file)\n",
    "#         office_file.load_key(password=password)\n",
    "\n",
    "#         with tempfile.NamedTemporaryFile(suffix=file_path.suffix, delete=False) as temp_file:\n",
    "#             office_file.decrypt(temp_file)\n",
    "#             return Path(temp_file.name)\n",
    "\n",
    "# def check_encryption_status(excel_files, password=\"\"):\n",
    "#     encrypted_files = []\n",
    "#     for file_type in excel_files:\n",
    "#         for file in excel_files[file_type]:\n",
    "#             if is_encrypted(file):\n",
    "#                 encrypted_files.append(file)\n",
    "\n",
    "#     if not encrypted_files:\n",
    "#         print(\"No encrypted files found.\")\n",
    "#     else:\n",
    "#         for file in encrypted_files:\n",
    "#             print(f\"{file} is encrypted.\")\n",
    "\n",
    "# check_encryption_status(excel_files, \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Part 3:\n",
    "\n",
    "        - If a file is encrypted, it is decrypted before attempting to read it\n",
    "        - Any issues in reading a workbook (perhaps due to incorrect decryption or file corruption) are caught and reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analyze_workbooks(excel_files, password=\"\"):\n",
    "#     workbook_info = []\n",
    "\n",
    "#     for file_type in excel_files:\n",
    "#         for file_path in excel_files[file_type]:\n",
    "#             if is_encrypted(file_path):\n",
    "#                 if not password:\n",
    "#                     print(f\"‚ö†Ô∏è Skipping encrypted file (no password provided): {file_path}\")\n",
    "#                     continue\n",
    "\n",
    "#                 print(f\"üîì Attempting to decrypt: {file_path}\")\n",
    "#                 try:\n",
    "#                     file_path = decrypt_excel(file_path, password)\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"‚ùå Decryption failed for {file_path}: {e}\")\n",
    "#                     continue\n",
    "\n",
    "#             try:\n",
    "#                 workbook = pd.ExcelFile(file_path)\n",
    "#                 for sheet_name in workbook.sheet_names:\n",
    "#                     workbook_info.append({\n",
    "#                         'WorkbookName': file_path.name,\n",
    "#                         'WorksheetName': sheet_name,\n",
    "#                         'PathName': file_path\n",
    "#                     })\n",
    "#             except Exception as e:\n",
    "#                 print(f\"‚ùå Error reading {file_path}: {e}\")\n",
    "\n",
    "#     return workbook_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Part 4:\n",
    "\n",
    "        - all_dfs is a list that will store all unique DataFrames.\n",
    "        - Each new sheet_df is compared with the DataFrames already in all_dfs.\n",
    "        - If a match is found, sheet_df is appended to the matching DataFrame.\n",
    "        - If no match is found, sheet_df is added as a new unique DataFrame to all_dfs.\n",
    "        - The function returns all_dfs, which contains all consolidated DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def same_columns(df1, df2):\n",
    "#     \"\"\"\n",
    "#     Check if two DataFrames have the same columns (ignoring order).\n",
    "#     \"\"\"\n",
    "#     return set(df1.columns) == set(df2.columns)\n",
    "\n",
    "# def consolidate_workbooks(workbook_info, password=\"\"):\n",
    "#     if not workbook_info:\n",
    "#         print(\"‚ùå No workbooks to consolidate.\")\n",
    "#         return []\n",
    "\n",
    "#     all_dfs = []\n",
    "#     file_timestamps = {}\n",
    "\n",
    "#     for info in workbook_info:\n",
    "#         workbook_path = info['PathName']\n",
    "#         sheet_name = info['WorksheetName']\n",
    "\n",
    "#         if is_encrypted(workbook_path):\n",
    "#             if not password:\n",
    "#                 print(f\"‚ö†Ô∏è Skipping encrypted file in consolidation (no password): {workbook_path}\")\n",
    "#                 continue\n",
    "#             try:\n",
    "#                 workbook_path = decrypt_excel(workbook_path, password)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"‚ùå Failed to decrypt during consolidation: {workbook_path}: {e}\")\n",
    "#                 continue\n",
    "\n",
    "#         try:\n",
    "#             sheet_df = pd.read_excel(workbook_path, sheet_name=sheet_name, header=0, dtype=str, engine='openpyxl')\n",
    "#         except Exception as e:\n",
    "#             print(f\"‚ùå Failed to read sheet {sheet_name} in {workbook_path}: {e}\")\n",
    "#             continue\n",
    "\n",
    "#         sheet_df['DTT_FILENAME'] = str(workbook_path)\n",
    "\n",
    "#         matched = False\n",
    "#         for i, df in enumerate(all_dfs):\n",
    "#             if same_columns(df.iloc[:, 1:], sheet_df.iloc[:, 1:]):  # Ignore index col\n",
    "#                 start_id = len(df) + 1\n",
    "#                 sheet_df['DTT_ID'] = range(start_id, start_id + len(sheet_df))\n",
    "#                 all_dfs[i] = pd.concat([df, sheet_df], ignore_index=True)\n",
    "#                 matched = True\n",
    "#                 break\n",
    "\n",
    "#         if not matched:\n",
    "#             sheet_df['DTT_ID'] = range(1, len(sheet_df) + 1)\n",
    "#             all_dfs.append(sheet_df)\n",
    "\n",
    "#     return all_dfs\n",
    "\n",
    "# # Prompt for password interactively if needed\n",
    "# from getpass import getpass\n",
    "\n",
    "# # Check if any files are encrypted\n",
    "# password = \"FBT2025\"\n",
    "# if any(is_encrypted(f) for ft in excel_files for f in excel_files[ft]):\n",
    "#     password = getpass(\"üîë Enter password to decrypt encrypted files: \")\n",
    "\n",
    "# # Run safely\n",
    "# workbook_info = analyze_workbooks(excel_files, password)\n",
    "# all_dfs = consolidate_workbooks(workbook_info, password)\n",
    "\n",
    "# # Display results\n",
    "# for i, df in enumerate(all_dfs):\n",
    "#     print(f\"\\nDataFrame {i+1} Preview:\")\n",
    "#     display(df.head(5))\n",
    "#     print(f\"Total rows in DataFrame {i+1}: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove double quotes and trim trailing space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def double_quotes_and_trim_dataframes(dfs):\n",
    "#     trimmed_dfs = []  # List to store cleaned DataFrames\n",
    "\n",
    "#     for df in dfs:\n",
    "#         # Create a copy of the DataFrame\n",
    "# #        df_copy = df.copy()\n",
    "\n",
    "#         # Select columns of object type\n",
    "#         cols = df.select_dtypes(['object']).columns\n",
    "\n",
    "#         # Apply the string operations only to string values\n",
    "#         for col in cols:\n",
    "#             df[col] = df[col].apply(lambda x: x.replace('\"', '').strip() if isinstance(x, str) else x)\n",
    "        \n",
    "#         trimmed_dfs.append(df)\n",
    "\n",
    "#     return trimmed_dfs\n",
    "\n",
    "# # Apply the cleaning process to all DataFrames\n",
    "# trimmed_all_dfs = double_quotes_and_trim_dataframes(all_dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define local file path\n",
    "# local_output_file = r\"C:\\Users\\ele\\Downloads\\PMC\\trimmed_all_dfs.xlsx\"\n",
    "\n",
    "# # Save the DataFrame to an Excel file locally\n",
    "# trimmed_all_dfs[0].to_excel(local_output_file, index=False, sheet_name=\"Processed Data\")\n",
    "\n",
    "# print(f\"File saved locally at: {local_output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
