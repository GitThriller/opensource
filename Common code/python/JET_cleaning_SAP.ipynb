{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard SAP JET Cleaning Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statement\n",
    "     SAP ERP typically generates two files for JET analysis purpose\n",
    "         1. BKPF - Header file\n",
    "         2. BSEG - Journal Line file\n",
    "     due to size issue, sometimes, audit team will provide mutiple files for each kind, \n",
    "     for instance in this example, there will be 2 BKPF files and 2 BSEG files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Essential Packages\n",
    "    These are common libraries and packages used across most SAP jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameID:\n",
    "    def __init__(self, df):\n",
    "        self.df = df_headers\n",
    "        \n",
    "    def AddID(self):\n",
    "        self.df['DTT_ID'] = np.arange(len(df_headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load all header tables\n",
    "\n",
    "aka. BKPF\n",
    "\n",
    "The following code can handle any number of header files.\n",
    "\n",
    "Practically, for any DS jobs, the first thing to do is see if data can be loaded properly, if there are errors loading, more\n",
    "precleaning is needed (e.g. fixes for delimiters or unexpected newlines -- see fix_delimiter.py in Common Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_path = r'C:\\PlaceWhereFilesAre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract just the header files from the raw data\n",
    "# Update this path to match the location of the input files and simplify the glob if they don't have \"JEHeaders\" file names\n",
    "headers_dir=glob(r'')\n",
    "headers_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import header tables\n",
    "headers=[pd.read_csv(f,sep = ',',dtype = str,encoding='utf-8',header=0,index_col=None, engine='c'                     \n",
    "                     #quoting=csv.QUOTE_NONE,\n",
    "                   #error_bad_lines=False,engine='python'\n",
    "                  ) for f in headers_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_headers=pd.concat(headers, ignore_index=True)\n",
    "df_headers.shape # rowcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_headers.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load all journal line tables\n",
    "\n",
    "aka. BSEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the line files from the raw data\n",
    "# Update this path to match the location of the input files and simplify the glob if they don't have \"JEDetails\" file names\n",
    "lines_dir=glob(r'{input_data_path}\\je_JEDetails_*.txt')\n",
    "lines_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import line tables\n",
    "lines=[pd.read_csv(f,\n",
    "                   sep = '|', # change to correct delimiter where needed\n",
    "                   dtype = str,\n",
    "                   encoding='utf-8', # try 'windows-1252' if 'utf-8' returns error\n",
    "                   header=0, # can comment out where needed\n",
    "                   index_col=None,\n",
    "                   engine='c',\n",
    "                   quoting=csv.QUOTE_NONE, # If there are quotes used as text qualifiers, change this setting\n",
    "#                   error_bad_lines=False, # used to check and return bad lines\n",
    "#                   engine='python', # used to return bad lines with clear line by line formality\n",
    "#                   low_memory=False # if engine='c' is on, low_memory needs to be commented off\n",
    "                  ) for f in lines_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lines=pd.concat(lines, ignore_index=True) # transform list to dataframe\n",
    "df_lines.shape # original rowcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lines.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove leading and trailing space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_headers.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lines.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all leading and trailing space for all values in all columns for both headers and lines\n",
    "cols_headers=df_headers.select_dtypes(['object']).columns\n",
    "df_headers[cols_headers]=df_headers[cols_headers].apply(lambda x: x.str.strip())\n",
    "\n",
    "cols_lines=df_lines.select_dtypes(['object']).columns\n",
    "df_lines[cols_lines]=df_lines[cols_lines].apply(lambda x: x.str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove space in column headers in both headers and lines\n",
    "df_headers.columns = df_headers.columns.str.replace(' ', '') \n",
    "\n",
    "df_lines.columns = df_lines.columns.str.replace(' ', '') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_headers.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lines.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sense check\n",
    "df_lines[df_lines['SHKZG']=='H'].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join header to journal line tables\n",
    "\n",
    "Ideally, join header and line tables right after loading raw header and line tables.\n",
    "\n",
    "Unless both tables involve large number of columns (like 250+ columns) and massive number of rows (50 million rows)\n",
    "you then might subset raw data based on column mapping to reduce the size of file and then join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join tables and check rowcounts\n",
    "df_joined=pd.merge(df_lines, df_headers, how='left', # left or inner join depending on the needs\n",
    "                   left_on=['MANDT','BUKRS','BELNR','GJAHR'],\n",
    "                   right_on=['MANDT','BUKRS','BELNR','GJAHR'],\n",
    "                   suffixes=('', '_y'))\n",
    "df_joined.shape # rowcount should be the same as line table original rowcount above unless there are filtering conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values and how many there are in each column\n",
    "df_joined.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locate Essential Paths\n",
    "\n",
    "Set local file paths from this step forward, as the following file circulation will be consistently \n",
    "saved under one of those paths.\n",
    "\n",
    "This is the only work directory (path) needed to be changed to your current master path if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get master file path and output path\n",
    "master_path=Path(r'C:\\Users\\...') # must change the path depending where you put your data\n",
    "output_path=master_path/'3.Cleaned' # adjust the folder name where needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Column Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step does not affect the rest of the script, it's just to remind you to look at the column mapping\n",
    "# hard coded path, no need to change this path to bring out the standard SAP column mapping\n",
    "col_mapping_file = pd.read_excel(r'')\n",
    "col_mapping_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list including needed column names]\n",
    "col_header_name = ['BUKRS', # COMPANY_CODE\n",
    "                   'BUZEI', # LINE_ITEM\n",
    "#                   'DTT_ID', # only needed when BUZEI not unique\n",
    "                   'Journal ID', # Journal ID is consist of 4 columns\n",
    "                   'Journal description', # Journal desciption is consist of 2 columns with conditions\n",
    "                   'HKONT', # GL_ACCOUNT_NUMBER\n",
    "                   'Amount', # DMBTR * SHKZG\n",
    "                   'BUDAT', # POSTING_DATE\n",
    "                   'CPUDT', # ENTRY_DATE\n",
    "                   'CUKY', # optional\n",
    "                   'Foreign currency amount', # WRBTR * SHKZG\n",
    "                   'BLART', # DOCUMENT_TYPE\n",
    "                   'USNAM',\n",
    "                   'TCODE'\n",
    "                  ] # POSTING_USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_mapping = {'BUKRS': 'Entity',\n",
    "               'BUZEI': 'Journal line number',\n",
    "#               'DTT_ID': 'Journal line number manual'\n",
    "               'Journal ID': 'Journal ID',\n",
    "               'Journal description': 'Journal description',\n",
    "               'HKONT': 'GL account',\n",
    "               'Amount': 'Amount',\n",
    "               'BUDAT': 'Date effective',\n",
    "               'CPUDT': 'Date posted',\n",
    "               'CUKY': 'Currency code',\n",
    "               'Foreign currency amount': 'Foreign currency amount',\n",
    "               'BLART': 'Document type',\n",
    "               'USNAM': 'Posting user',\n",
    "               'TCODE': 'Transaction code'\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_header = list(col_mapping.values())\n",
    "col_header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle Data\n",
    "\n",
    "This section is to wrangle and transform data into required format.\n",
    "Pre-validation also included in this section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deal with date format and pre validate date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check date effective range\n",
    "print(pd.to_datetime(df_joined['BUDAT']).min(), \n",
    "      pd.to_datetime(df_joined['BUDAT']).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if looks irregular format or date range not making sense\n",
    "# format\n",
    "df_joined['BUDAT']=df_joined['BUDAT'].astype('datetime64') # normally .astype should work perfectly\n",
    "\n",
    "# otherwise, can use pd.to_datetime with more detailed formating and add errors='coerce' where needed\n",
    "# df_joined['BUDAT']=df_joined['BUDAT'].apply(pd.to_datetime,format='%Y%m%d') # format needs to be altered where needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check date effective range again after conversion\n",
    "print(df_joined['BUDAT'].min(), \n",
    "      df_joined['BUDAT'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also check date posted format\n",
    "print(pd.to_datetime(df_joined['CPUDT']).min(), \n",
    "      pd.to_datetime(df_joined['CPUDT']).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if looks irregular format or date range not making sense\n",
    "# format\n",
    "df_joined['CPUDT']=df_joined['CPUDT'].astype('datetime64') # normally .astype should work perfectly\n",
    "\n",
    "# otherwise, can use pd.to_datetime with more detailed formating and add errors='coerce' where needed\n",
    "# df_joined['BUDAT']=df_joined['BUDAT'].apply(pd.to_datetime,format='%Y%m%d') # format needs to be altered where needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check date posted format again after conversion\n",
    "print(df_joined['CPUDT'].min(), \n",
    "      df_joined['CPUDT'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deal with amount and foreign currency amount format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in DMBTR column\n",
    "df_joined[df_joined['DMBTR'].isnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert DBMTR as float\n",
    "df_joined['DMBTR']=df_joined['DMBTR'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert indicator and create Amount column\n",
    "df_joined['indicator']=df_joined['SHKZG'].apply(lambda x: -1 if x =='H' else 1 )\n",
    "df_joined['Amount']=df_joined['indicator']*df_joined['DMBTR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check netting to 0\n",
    "round(df_joined['Amount'].sum(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same process for foreign currency amount\n",
    "\n",
    "df_joined['WRBTR']=df_joined['WRBTR'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined['Foreign currency amount']=df_joined['indicator']*df_joined['WRBTR'] \n",
    "# it may have more conditions when calculating foreign currency amount, so pay attention and modify accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### form correct journal ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any missing values in journal ID major column\n",
    "df_joined[df_joined['BELNR'].isnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate columns to form journal ID BELNR\n",
    "df_joined['Journal ID']=df_joined['MANDT'].map(str) + '_' + df_joined['BUKRS'].map(str) + '_' + df_joined['BELNR'].map(str) + '_' + df_joined['GJAHR'].map(str)\n",
    "df_joined['Journal ID'].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### form correct journal description column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate columns to form journal description\n",
    "df_joined['Journal description']=df_joined['BKTXT'] + ' ' + df_joined_anx['SGTXT']\n",
    "df_joined[df_joined['Journal description'].notnull()].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove double quotes in journal description\n",
    "df_joined['Journal description'] = df_joined['Journal description'].str.replace('\"','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assign 'N/A' as value only when there is no BLART column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_joined['BLART']='N/A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assign 'N/A' as value only when there is no USNAM column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_joined['USNAM']='N/A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map columns to subset JET required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset dataframe for myA required columns\n",
    "df_subset=df_joined[col_header_name].rename(columns=col_mapping)[col_header]\n",
    "df_subset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review of data types of each column after formatting\n",
    "df_subset.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check data range again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_subset['Date effective'].min(), df_subset['Date effective'].max())\n",
    "# within required analysis range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check netting to 0 again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df_subset['Amount'].sum(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check netting to 0 by each Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_by_entity=df_subset.groupby('Entity').agg({'Amount': 'sum'})\n",
    "round(sum_by_entity,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check nets to 0 by each journal ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net by [Journal ID]\n",
    "net_by_jID=round(df_subset.groupby('Journal ID').agg({'Journal line number':'count',\n",
    "                                                      'Amount': 'sum'}),2)\n",
    "net_by_jID.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring unbalanced journal id out\n",
    "unbalanced=net_by_jID[net_by_jID['Amount'].abs()>0].rename(columns={'Journal line number':'Count',\n",
    "                                                                        'Amount':'Sum'})\n",
    "unbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplications on [Journal ID] and [Journal Line Number]\n",
    "check_unique=df_subset.groupby(['Journal ID','Journal line number']).size()\n",
    "check_unique.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring up duplications\n",
    "check_unique[check_unique>1]#.to_frame().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    if large data size, group all aggregations together will take fairly long time to run, so can run it separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check uniqueness upon grouped journal ID\n",
    "# unique_cols=df_subset.groupby('Journal ID').agg({'Document type': lambda w: w.nunique(),\n",
    "#                                                  'Posting user': lambda x: x.nunique(),\n",
    "#                                                  'Date effective': lambda y: y.nunique(),\n",
    "#                                                  'Date posted': lambda z: z.nunique()})\n",
    "# unique_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_jID_doc=df_subset.groupby('Journal ID').agg({'Document type': lambda w: w.nunique()})\n",
    "unique_jID_doc[unique_jID_doc['Document type']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_jID_user=df_subset.groupby('Journal ID').agg({'Posting user': lambda x: x.nunique()})\n",
    "unique_jID_user[unique_jID_user['Posting user']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_jID_e_date=df_subset.groupby('Journal ID').agg({'Date effective': lambda y: y.nunique()})\n",
    "unique_jID_e_date[unique_jID_e_date['Date effective']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_jID_p_date=df_subset.groupby('Journal ID').agg({'Date posted': lambda z: z.nunique()})\n",
    "unique_jID_p_date[unique_jID_p_date['Date posted']>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check double quotes\n",
    "\n",
    "For large datasets, if double quotes have been addressed prior loading in, can skip this step as it could take very long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check double quotes in all columns\n",
    "check_quotes=np.column_stack([df_subset[col].astype(str).str.contains('\"', na=False) for col in df_subset])\n",
    "df_subset.loc[check_quotes.any(axis=1)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check if any Irregular GL account format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overview all GL account to spot any irregular GL account format\n",
    "df_subset['GL account'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate EXP Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re order columns if needed\n",
    "df_final=df_subset.reindex(columns=col_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final rowcount check\n",
    "df_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### movement by gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sum_by_gl.xlsx\n",
    "sum_by_gl=df_final.groupby('GL account').agg({'Journal line number': 'count',\n",
    "                                              'Amount': 'sum'}\n",
    "                                            ).rename(columns = {'Journal line number':'Count',\n",
    "                                                                'Amount':'Sum'})\n",
    "sum_by_gl['Sum']=round(sum_by_gl['Sum'],2)\n",
    "sum_by_gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "sum_by_gl.to_excel(output_path/'sum_by_gl.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### document types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_type=df_final.groupby('Document type').agg({'Journal ID': lambda x: x.nunique(),\n",
    "                                                'Journal line number': 'count',\n",
    "                                                'Amount': 'sum'}\n",
    "                                              ).rename(columns = {'Journal ID':'Number of entries',\n",
    "                                                                  'Journal line number':'Number of lines',\n",
    "                                                                  'Amount':'Total amount'}).reset_index()\n",
    "doc_type['Is standard document type'] = 0\n",
    "doc_type['Total amount']=round(doc_type['Total amount'],2)\n",
    "doc_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "doc_type.to_csv(output_path/'document_type.csv',quoting=csv.QUOTE_ALL,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### posting user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posting_user=df_final.groupby('Posting user').agg({'Journal ID': lambda x: x.nunique(),\n",
    "                                                   'Journal line number': 'count',\n",
    "                                                   'Amount': 'sum'}\n",
    "                                                 ).rename(columns = {'Journal ID':'Number of entries',\n",
    "                                                                     'Journal line number':'Number of lines',\n",
    "                                                                     'Amount':'Total amount'}).reset_index()\n",
    "posting_user['Is system entry'] = 0\n",
    "posting_user['User of interest'] = 0\n",
    "posting_user['Total amount']=round(posting_user['Total amount'],2)\n",
    "posting_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "posting_user.to_csv(output_path/'posting_user.csv',quoting=csv.QUOTE_ALL,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### journal lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final review of journal lines\n",
    "df_final.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "df_final.to_csv(output_path/'journal_lines.csv',quoting=csv.QUOTE_ALL,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
