{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming approach\n",
    "\n",
    "- Lists vs generators\n",
    "- Pandas streaming approach\n",
    "- Pandas + pickle\n",
    "- Pandas methods vs python in built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the base of pandas - provides high performance narray storage (much faster than python lists)\n",
    "import numpy as np\n",
    "\n",
    "# pandas builds on top of numpy to provide extended features (data)\n",
    "import pandas as pd\n",
    "\n",
    "# e.g. can see numpy in pandas source code\n",
    "# https://github.com/pandas-dev/pandas/blob/master/pandas/core/arrays/masked.py\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lists vs generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [1,2,3,4,5]\n",
    "\n",
    "# define a new list with list comprehension\n",
    "nums2_list = [ n**2 for n in nums ]\n",
    "\n",
    "# define a new generator with genexp\n",
    "nums2_gen = ( n**2 for n in nums )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guess the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums2_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums2_gen[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums2_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nums2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nums2_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in nums2_list:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in nums2_gen:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in nums2_gen:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(nums2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(nums2_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas streaming approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(os.getcwd()) / 'data'\n",
    "\n",
    "input_path = data_dir / 'journals-large.csv'\n",
    "output_path = data_dir / 'journals-large-out.csv'\n",
    "\n",
    "if (output_path.is_file()):\n",
    "    print('output file exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_csv(\n",
    "    input_path,\n",
    "    chunksize=200000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = next(chunks)\n",
    "chunk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunk.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del chunks, chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_csv(\n",
    "    input_path,\n",
    "    chunksize=1000000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_diff(start_time):\n",
    "    return round((dt.now() - start_time).total_seconds(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_output(chunk, headers: bool):\n",
    "    \n",
    "    # whatever processing you need per chunk in here...\n",
    "    \n",
    "    start_calcs = dt.now()\n",
    "    # remove Journal from journal ID column\n",
    "    chunk.loc[:, 'Journal ID clean'] = chunk.loc[:, 'Journal ID'].str.replace('Journal ', '', regex=False)\n",
    "    \n",
    "    # divide amount by 100 \n",
    "    chunk.loc[:, 'Amount divided'] = chunk.loc[:, 'Amount'] / 100\n",
    "    \n",
    "    print(f'Calculations:\\t{time_diff(start_calcs)}')\n",
    "    \n",
    "    grouped = chunk.groupby(['Journal ID'])\n",
    "    \n",
    "    start_output = dt.now()\n",
    "    # doesn't have to be here - could return the chunk and output elsewhere\n",
    "    chunk.to_csv(\n",
    "        output_path,\n",
    "        index=False,      # don't output the index\n",
    "        header=headers,   # only output headers when this is true (first chunk)\n",
    "        mode='a',         # APPEND - need this or each chunk will override - CHECK FILE DOESN'T EXIST ALREADY\n",
    "    )    \n",
    "    \n",
    "    print(f'Disk output:\\t{time_diff(start_output)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = dt.now()\n",
    "row_count = 0\n",
    "\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    \n",
    "    # next chunk is loaded into memory\n",
    "    start_chunk = dt.now()\n",
    "    \n",
    "    # add to row count\n",
    "    row_count += len(chunk.index)\n",
    "    \n",
    "    # do whatever we need to with this chunk\n",
    "    print(f'Processing chunk {idx+1}')\n",
    "    process_and_output(chunk, idx == 0)\n",
    "    print(f'Chunk complete:\\t{time_diff(start_chunk)}\\n')\n",
    "    \n",
    "    # when the loop goes to the next chunk, the reference to the previous is removed (python will garbage collect it)\n",
    "    \n",
    "print(f'Processed {row_count} rows in {time_diff(start)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disk operations vs in memory\n",
    "\n",
    "- Reading or writing to disk is **orders of magitude** slower than in memory operations (e.g. on the CPU or GPU)\n",
    "- Optimise streaming approach to have as much in memory as possible at a time\n",
    "\n",
    "## Pandas optimised functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'Amount'] = df.loc[:, 'Amount'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit df['Amount']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide(amount):\n",
    "    return amount /10\n",
    "\n",
    "%timeit df['Amount'].apply(divide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas pickle\n",
    "\n",
    "- Faster and somewhat compressed data storage (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = data_dir / 'journal-pickle.pkl'\n",
    "df.to_pickle(data_dir / 'journal-pickle.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pd.read_pickle(data_dir / 'journal-pickle.pkl')\n",
    "# 5.22 s ± 26.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pd.read_csv(output_path)\n",
    "# 14.7 s ± 144 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other libraries\n",
    "\n",
    "- Dask! - keen to explore, can do grouping and aggregate calculations with data on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
